from fastapi import FastAPI, Body, UploadFile, File, HTTPException, Query, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse, HTMLResponse
from typing import List, Dict, Optional, Union
import json
import os
import gzip
import base64
from datetime import datetime, timedelta
import requests
from pydantic import BaseModel, ValidationError
from google.auth.transport.requests import Request as GoogleAuthRequest
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
import io
import logging
from sqlalchemy.orm import Session
from sqlalchemy.sql import func
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import parsing functions from our parser module
from parser import parse_schedules, parse_schedules_classic_only, parse_schedules_ai_only

# Import database modules
from database import get_database, ScheduleService, create_tables, test_connection, run_migrations, SessionLocal, Schedule, Tag, User

# --- App Initialization ---
app = FastAPI()

# --- Logger Initialization ---
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# --- Database Initialization ---
@app.on_event("startup")
async def startup_event():
    """Initialize database on startup"""
    print("ğŸ”„ Initializing database...")
    try:
        # Test connection
        if test_connection():
            print("âœ… Database connection established")

            # Create tables if they don't exist
            create_tables()

            # Run database migrations
            run_migrations()

            print("âœ… Database initialization complete")
        else:
            print("âŒ Database connection failed")
    except Exception as e:
        print(f"âŒ Database initialization failed: {e}")

# --- CORS Configuration ---
# This allows the frontend to communicate with the backend.
origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:5174",
    "http://127.0.0.1:5174",
    "http://localhost:5175",
    "http://127.0.0.1:5175",
]

# Add production frontend URL if specified
FRONTEND_URL = os.getenv('FRONTEND_URL')
if FRONTEND_URL:
    origins.append(FRONTEND_URL)

# Railway í™˜ê²½ì—ì„œëŠ” ëª¨ë“  Railway ë„ë©”ì¸ í—ˆìš© (í”„ë¡œë•ì…˜, í…ŒìŠ¤íŠ¸ ë“±)
if os.getenv('RAILWAY_STATIC_URL') or os.getenv('RAILWAY_GIT_BRANCH'):
    origins.extend([
        "https://kakaotalk-schedule-parser.up.railway.app",
        "https://bs-snaper-test.up.railway.app",
        "https://bs-snaper.up.railway.app",
		"https://bssnaper.enfree.com",
		"https://bonsiksched.enfree.com",
		"https://schedule-parser-backend-test.up.railway.app"
    ])

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Validation Error Handler ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(f"âŒ Validation error on {request.method} {request.url.path}")
    print(f"ğŸ” Error details: {exc.errors()}")

    # Log the request body for debugging
    try:
        body = await request.body()
        print(f"ğŸ“„ Request body: {body.decode('utf-8')}")
    except Exception as e:
        print(f"âš ï¸ Could not read request body: {e}")

    return JSONResponse(
        status_code=422,
        content={
            "error": "Validation failed",
            "detail": exc.errors(),
            "success": False
        }
    )

# --- OAuth Configuration ---
# Google
GOOGLE_CLIENT_ID = os.getenv('GOOGLE_CLIENT_ID')
GOOGLE_CLIENT_SECRET = os.getenv('GOOGLE_CLIENT_SECRET')
DEV_ADMIN_ID = os.getenv('VITE_DEV_ADMIN_ID')  # ê°œë°œì ê´€ë¦¬ì ID

if not GOOGLE_CLIENT_ID or not GOOGLE_CLIENT_SECRET:
    raise ValueError("GOOGLE_CLIENT_IDì™€ GOOGLE_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Naver
NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET')

# Kakao
KAKAO_REST_API_KEY = os.getenv('KAKAO_REST_API_KEY')
KAKAO_CLIENT_SECRET = os.getenv('KAKAO_CLIENT_SECRET')

# Railway í™˜ê²½ ìë™ ê°ì§€ ë° redirect URI ì„¤ì •
if os.getenv('RAILWAY_STATIC_URL') or os.getenv('RAILWAY_GIT_BRANCH'):
    # Railway ë°°í¬ í™˜ê²½ - GIS ë¦¬ë‹¤ì´ë ‰íŠ¸ ëª¨ë“œ
    FRONTEND_URL = os.getenv('FRONTEND_URL', 'https://your-app.railway.app')
    GOOGLE_REDIRECT_URI = f'{FRONTEND_URL}/auth/callback.html'
    NAVER_REDIRECT_URI = f'{FRONTEND_URL}/auth/naver/callback'
    KAKAO_REDIRECT_URI = f'{FRONTEND_URL}/auth/kakao/callback'
else:
    # ë¡œì»¬ ê°œë°œ í™˜ê²½ - GIS ë¦¬ë‹¤ì´ë ‰íŠ¸ ëª¨ë“œ
    GOOGLE_REDIRECT_URI = 'http://localhost:5173/auth/callback.html'
    NAVER_REDIRECT_URI = 'http://localhost:5173/auth/naver/callback'
    KAKAO_REDIRECT_URI = 'http://localhost:5173/auth/kakao/callback'


# --- Storage Configuration ---
# ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì˜ ë°±ì—”ë“œ ë¡œì»¬ ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: ë¹„í™œì„±í™”, êµ¬ê¸€ë“œë¼ì´ë¸Œë§Œ ì‚¬ìš©)
ENABLE_LOCAL_BACKUP = os.getenv('ENABLE_LOCAL_BACKUP', 'false').lower() == 'true'

# --- Data Models ---
class GoogleAuthRequest(BaseModel):
    code: str
    redirect_uri: str = None  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•œ redirect_urië¥¼ ë°›ìŒ

class NaverAuthRequest(BaseModel):
    code: str
    state: str

class KakaoAuthRequest(BaseModel):
    code: str

class SaveSchedulesRequest(BaseModel):
    schedules: Union[List[Dict], str]  # ì••ì¶•ëœ ë¬¸ìì—´ë„ í—ˆìš©
    user_id: str
    access_token: Optional[str] = None
    refresh_token: Optional[str] = None
    device_uuid: Optional[str] = None

class LoadSchedulesRequest(BaseModel):
    user_id: str
    access_token: str
    refresh_token: Optional[str] = None

# Railway Persistent Storage Models
class PersistentSaveRequest(BaseModel):
    user_id: str
    schedules_data: Dict

class PersistentLoadRequest(BaseModel):
    user_id: str

class ParseTextRequest(BaseModel):
    text: str
    engine: str = "hybrid"  # classic, hybrid, ai_only

# --- Data File Path ---
# The path is relative to the root of the project where the server will be started.
DATA_FILE_PATH = '../.screenshot/KakaoTalk_20250814_1307_38_031_KPAG_ë§¤ë‹ˆì €.txt'
SCHEDULES_DATA_DIR = 'data'
SCHEDULES_DATA_FILE = 'schedules_latest.json'

# Railway Persistent Volume ì €ì¥ì†Œ ì„¤ì •
STORAGE_DIR = os.getenv('RAILWAY_VOLUME_MOUNT_PATH', 'storage')
USERS_DATA_DIR = os.path.join(STORAGE_DIR, 'users')

# Ensure directories exist
os.makedirs(SCHEDULES_DATA_DIR, exist_ok=True)
os.makedirs(USERS_DATA_DIR, exist_ok=True)

print(f"ğŸ“ Data directories initialized:")
print(f"  - Legacy data: {SCHEDULES_DATA_DIR}")
print(f"  - Storage: {STORAGE_DIR}")
print(f"  - Users data: {USERS_DATA_DIR}")

# --- Railway Persistent Storage Helper Functions ---
def get_user_data_dir(user_id: str) -> str:
    """Get the directory path for a specific user"""
    user_dir = os.path.join(USERS_DATA_DIR, user_id)
    os.makedirs(user_dir, exist_ok=True)
    return user_dir

def save_to_persistent_storage(user_id: str, schedules_data: dict) -> bool:
    """Save schedules to Railway Persistent Volume"""
    try:
        user_dir = get_user_data_dir(user_id)
        schedules_file = os.path.join(user_dir, 'schedules.json')
        metadata_file = os.path.join(user_dir, 'metadata.json')

        # Save schedules data with compression
        save_json_compressed(schedules_data, schedules_file.replace('.json', ''))

        # Save metadata
        metadata = {
            'user_id': user_id,
            'last_updated': datetime.utcnow().isoformat(),
            'schedules_count': len(schedules_data.get('schedules', [])),
            'version': '2.0',  # New persistent storage version
            'storage_type': 'railway_persistent'
        }

        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"âœ… Persistent storage save successful ({metadata['schedules_count']} schedules)")
        return True

    except Exception as e:
        print(f"âŒ Persistent storage save failed: {e}")
        return False

def load_from_persistent_storage(user_id: str) -> Optional[dict]:
    """Load schedules from Railway Persistent Volume"""
    try:
        user_dir = get_user_data_dir(user_id)
        schedules_file = os.path.join(user_dir, 'schedules.json')
        metadata_file = os.path.join(user_dir, 'metadata.json')

        # Check for compressed file first, fallback to regular file
        compressed_file = schedules_file.replace('.json', '.gz')

        if os.path.exists(compressed_file):
            print(f"ğŸ“¦ Loading compressed data")
            schedules_data = load_json_compressed(schedules_file.replace('.json', ''))
        elif os.path.exists(schedules_file):
            print(f"ğŸ“„ Loading uncompressed data")
            with open(schedules_file, 'r', encoding='utf-8') as f:
                schedules_data = json.load(f)
        else:
            print(f"ğŸ“ No persistent data found")
            return None

        # Load metadata if exists
        metadata = {}
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

        result = {
            'schedules': schedules_data.get('schedules', []),
            'metadata': metadata,
            'user_id': user_id,
            'source': 'railway_persistent'
        }

        print(f"âœ… Persistent storage load successful ({len(result['schedules'])} schedules)")
        return result

    except Exception as e:
        print(f"âŒ Persistent storage load failed: {e}")
        return None

def get_persistent_storage_status(user_id: str) -> dict:
    """Get persistent storage status for a user"""
    try:
        user_dir = get_user_data_dir(user_id)
        schedules_file = os.path.join(user_dir, 'schedules.json')
        compressed_file = schedules_file.replace('.json', '.gz')
        metadata_file = os.path.join(user_dir, 'metadata.json')

        # Check for either compressed or uncompressed file
        has_schedules = os.path.exists(compressed_file) or os.path.exists(schedules_file)

        status = {
            'user_id': user_id,
            'has_data': has_schedules,
            'has_metadata': os.path.exists(metadata_file),
            'last_updated': None,
            'schedules_count': 0,
            'storage_type': 'railway_persistent'
        }

        if status['has_metadata']:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                status.update(metadata)

        return status

    except Exception as e:
        print(f"âŒ Failed to get persistent storage status: {e}")
        return {'user_id': user_id, 'error': str(e)}

# --- Google Drive Helper Functions ---
def get_drive_service(access_token, refresh_token=None):
    """Create Google Drive service using access token and refresh token"""
    try:
        # Create credentials with all necessary fields
        credentials = Credentials(
            token=access_token,
            refresh_token=refresh_token,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET
        )

        # Check if token needs refresh
        print(f"ğŸ” Token valid: {credentials.valid}, Has refresh token: {bool(refresh_token)}")

        # Try to refresh the token if it's expired
        if refresh_token and not credentials.valid:
            try:
                print(f"ğŸ”„ Refreshing expired access token...")
                credentials.refresh(GoogleAuthRequest())
                print(f"âœ… Access token refreshed successfully")
                # Token refreshed successfully
            except Exception as refresh_error:
                # Failed to refresh token
                # Continue with original token, might still work
                pass
        elif not refresh_token:
            print(f"âš ï¸ No refresh token available for token refresh")
        else:
            print(f"ğŸŸ¢ Token is still valid, no refresh needed")

        # Build and return the service
        service = build('drive', 'v3', credentials=credentials)
        print(f"âœ… Google Drive service created successfully")
        return service
    except Exception as e:
        print(f"âŒ Failed to create Drive service: {e}")
        raise e

def save_to_drive_direct(access_token, folder_id, filename, data, log_func=print):
    """Direct HTTP API call to Google Drive with compression and metadata"""
    import requests
    from datetime import datetime

    log_func(f"ğŸ” Direct Drive API call with token: {access_token[:20]}...")

    try:
        # 1. ë°ì´í„° ì••ì¶•
        log_func(f"ğŸ“¦ ë°ì´í„° ì••ì¶• ì¤‘...")
        compressed_data, original_size, compressed_size = compress_json_data(data)

        # 2. ë©”íƒ€ë°ì´í„° ìƒì„±
        timestamp = datetime.now()
        version = timestamp.strftime("%Y%m%d_%H")
        schedules = data.get("schedules", [])
        user_id = data.get("user_id", "unknown")

        metadata = create_metadata(schedules, timestamp, version, user_id, original_size, compressed_size, request.device_uuid)

        # 3. ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¨¼ì € ì €ì¥
        log_func(f"ğŸ“ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
        meta_id = save_metadata_to_drive(access_token, folder_id, filename, metadata, log_func)

        if not meta_id:
            log_func(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            return None

        # 4. ì••ì¶•ëœ ë°ì´í„° íŒŒì¼ ì €ì¥
        log_func(f"ğŸ—œï¸ ì••ì¶• ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
        compressed_filename = filename.replace('.json', '.gz.b64')
        file_metadata = {
            'name': compressed_filename,
            'parents': [folder_id] if folder_id else []
        }

        headers = {'Authorization': f'Bearer {access_token}'}

        files = {
            'metadata': (None, json.dumps(file_metadata), 'application/json'),
            'media': (compressed_filename, compressed_data, 'text/plain')
        }

        response = requests.post(
            'https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart',
            headers=headers,
            files=files
        )

        log_func(f"ğŸ” Drive API response status: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            file_id = result.get('id')
            log_func(f"âœ… ì••ì¶• ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_id}")
            log_func(f"âœ… ë©”íƒ€ë°ì´í„° + ë°ì´í„° ì €ì¥ ì„±ê³µ")
            return file_id
        else:
            log_func(f"âŒ Drive API error: {response.status_code} - {response.text}")
            return None

    except Exception as e:
        log_func(f"âŒ Direct Drive upload failed: {type(e).__name__}: {e}")
        return None

def find_or_create_folder_direct(access_token, folder_name, log_func=print):
    """Direct API call to find or create folder"""
    import requests

    try:
        headers = {
            'Authorization': f'Bearer {access_token}'
        }

        # Search for existing folder
        search_url = 'https://www.googleapis.com/drive/v3/files'
        params = {
            'q': f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false",
            'fields': 'files(id, name)'
        }

        response = requests.get(search_url, headers=headers, params=params)

        if response.status_code == 200:
            files = response.json().get('files', [])
            if files:
                folder_id = files[0]['id']
                log_func(f"âœ… Found existing folder: {folder_id}")
                return folder_id

        # Create new folder
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }

        response = requests.post(
            'https://www.googleapis.com/drive/v3/files',
            headers={**headers, 'Content-Type': 'application/json'},
            data=json.dumps(folder_metadata)
        )

        if response.status_code == 200:
            folder = response.json()
            folder_id = folder.get('id')
            log_func(f"âœ… Created new folder: {folder_id}")
            return folder_id
        else:
            log_func(f"âŒ Folder creation failed: {response.status_code} - {response.text}")
            return None

    except Exception as e:
        log_func(f"âŒ Folder operation failed: {type(e).__name__}: {e}")
        return None

def find_or_create_app_folder(service):
    """Find or create the app folder in Google Drive"""
    folder_name = "Wedding Snapler Schedule Manager Data"

    try:
        print(f"ğŸ” Searching for folder: {folder_name}")
        # Search for existing folder
        query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
        results = service.files().list(q=query, fields="files(id, name)").execute()
        folders = results.get('files', [])

        if folders:
            print(f"âœ… Found existing folder: {folders[0]['id']}")
            return folders[0]['id']

        # Create new folder if not found
        print(f"ğŸ“ Creating new folder: {folder_name}")
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        folder = service.files().create(body=folder_metadata, fields='id').execute()
        folder_id = folder.get('id')
        print(f"âœ… Created folder with ID: {folder_id}")
        return folder_id
    except Exception as e:
        print(f"âŒ Folder creation/search failed: {e}")
        raise e

def save_to_drive(service, folder_id, filename, data):
    """Save JSON data to Google Drive"""
    try:
        # Convert data to JSON string
        json_data = json.dumps(data, ensure_ascii=False, indent=2)

        # Create file metadata
        file_metadata = {
            'name': filename,
            'parents': [folder_id]
        }

        # Check if file already exists
        query = f"name='{filename}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id)").execute()
        existing_files = results.get('files', [])

        # Create a temporary file to upload
        temp_file_path = f"/tmp/{filename}"
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(json_data)

        media = MediaFileUpload(temp_file_path, mimetype='application/json')

        if existing_files:
            # Update existing file
            file_id = existing_files[0]['id']
            file = service.files().update(
                fileId=file_id,
                media_body=media
            ).execute()
        else:
            # Create new file
            file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()

        # Clean up temp file
        os.remove(temp_file_path)

        return file.get('id')
    except Exception as e:
        print(f"Error saving to Drive: {e}")
        raise e

def save_to_drive_advanced(service, folder_id, filename, data, device_uuid=None, log_func=print):
    """Save JSON data to Google Drive with compression and metadata using Google client library"""
    from datetime import datetime
    from googleapiclient.http import MediaInMemoryUpload
    import io

    try:
        # 1. ë°ì´í„° ì••ì¶•
        log_func(f"ğŸ“¦ ë°ì´í„° ì••ì¶• ì¤‘...")
        compressed_data, original_size, compressed_size = compress_json_data(data)

        # 2. ë©”íƒ€ë°ì´í„° ìƒì„±
        timestamp = datetime.now()
        version = timestamp.strftime("%Y%m%d_%H")
        schedules = data.get("schedules", [])
        user_id = data.get("user_id", "unknown")

        metadata = create_metadata(schedules, timestamp, version, user_id, original_size, compressed_size, device_uuid)

        # 3. ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¨¼ì € ì €ì¥
        log_func(f"ğŸ“ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
        meta_filename = filename.replace('.json', '_metadata.json')
        meta_id = save_metadata_to_drive_service(service, folder_id, meta_filename, metadata, log_func)

        if not meta_id:
            log_func(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            return None

        # 4. ì••ì¶•ëœ ë°ì´í„° íŒŒì¼ ì €ì¥
        log_func(f"ğŸ—œï¸ ì••ì¶• ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
        compressed_filename = filename.replace('.json', '.gz.b64')

        # Create file metadata
        file_metadata = {
            'name': compressed_filename,
            'parents': [folder_id]
        }

        # Check if file already exists
        query = f"name='{compressed_filename}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id)").execute()
        existing_files = results.get('files', [])

        # Create media upload from compressed data
        media = MediaInMemoryUpload(compressed_data.encode('utf-8'), mimetype='text/plain')

        if existing_files:
            # Update existing file
            file_id = existing_files[0]['id']
            file = service.files().update(
                fileId=file_id,
                media_body=media
            ).execute()
            log_func(f"âœ… ê¸°ì¡´ íŒŒì¼ ì—…ë°ì´íŠ¸: {file_id}")
        else:
            # Create new file
            file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()
            file_id = file.get('id')
            log_func(f"âœ… ìƒˆ íŒŒì¼ ìƒì„±: {file_id}")

        log_func(f"âœ… ë©”íƒ€ë°ì´í„° + ì••ì¶• ë°ì´í„° ì €ì¥ ì„±ê³µ")
        return file.get('id')

    except Exception as e:
        log_func(f"âŒ ê³ ê¸‰ Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return None

def save_metadata_to_drive_service(service, folder_id, filename, metadata, log_func=print):
    """Save metadata file to Google Drive using service object"""
    from googleapiclient.http import MediaInMemoryUpload
    import json

    try:
        # Create file metadata
        file_metadata = {
            'name': filename,
            'parents': [folder_id]
        }

        # Check if file already exists
        query = f"name='{filename}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id)").execute()
        existing_files = results.get('files', [])

        # Convert metadata to JSON
        metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2)
        media = MediaInMemoryUpload(metadata_json.encode('utf-8'), mimetype='application/json')

        if existing_files:
            # Update existing file
            file_id = existing_files[0]['id']
            file = service.files().update(
                fileId=file_id,
                media_body=media
            ).execute()
            log_func(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—…ë°ì´íŠ¸: {file_id}")
        else:
            # Create new file
            file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()
            file_id = file.get('id')
            log_func(f"âœ… ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±: {file_id}")

        return file.get('id')

    except Exception as e:
        log_func(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def load_from_drive(service, folder_id, filename):
    """Load JSON data from Google Drive with compression support"""
    try:
        # ë¨¼ì € ì••ì¶•ëœ íŒŒì¼ ê²€ìƒ‰ (.gz.b64)
        compressed_filename = filename.replace('.json', '.gz.b64')
        query = f"name='{compressed_filename}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id, name)").execute()
        files = results.get('files', [])

        # ì••ì¶•ëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì›ë³¸ íŒŒì¼ ê²€ìƒ‰
        if not files:
            print(f"ğŸ“ ì••ì¶• íŒŒì¼ ì—†ìŒ, ì›ë³¸ íŒŒì¼ ê²€ìƒ‰: {filename}")
            query = f"name='{filename}' and '{folder_id}' in parents and trashed=false"
            results = service.files().list(q=query, fields="files(id, name)").execute()
            files = results.get('files', [])

            if not files:
                return None

            # ì›ë³¸ íŒŒì¼ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)
            file_id = files[0]['id']
            request = service.files().get_media(fileId=file_id)
            file_io = io.BytesIO()
            downloader = MediaIoBaseDownload(file_io, request)

            done = False
            while done is False:
                status, done = downloader.next_chunk()

            file_io.seek(0)
            content = file_io.read().decode('utf-8')
            return json.loads(content)

        # ì••ì¶•ëœ íŒŒì¼ ë¡œë“œ ë° í•´ì œ
        print(f"ğŸ“¦ ì••ì¶•ëœ íŒŒì¼ ë°œê²¬, í•´ì œ ì¤‘: {compressed_filename}")
        file_id = files[0]['id']

        # Download compressed file content
        request = service.files().get_media(fileId=file_id)
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)

        done = False
        while done is False:
            status, done = downloader.next_chunk()

        # ì••ì¶• í•´ì œ
        file_io.seek(0)
        compressed_b64 = file_io.read().decode('utf-8')
        return decompress_json_data(compressed_b64)

    except Exception as e:
        print(f"Error loading from Drive: {e}")
        return None

# --- Helper Functions ---
def calculate_checksum(schedules):
    """ìŠ¤ì¼€ì¤„ ë°ì´í„°ì˜ ì²´í¬ì„¬ ê³„ì‚°"""
    try:
        # ìŠ¤ì¼€ì¤„ì„ ì •ê·œí™”í•˜ê³  ì •ë ¬
        sorted_schedules = []
        for s in schedules:
            normalized = f"{s.get('date', '')}-{s.get('time', '')}-{s.get('couple', '')}"
            sorted_schedules.append(normalized)

        sorted_schedules.sort()
        combined = '|'.join(sorted_schedules)

        # ê°„ë‹¨í•œ í•´ì‹œ í•¨ìˆ˜ (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼)
        hash_value = 0
        for char in combined:
            hash_value = ((hash_value << 5) - hash_value) + ord(char)
            hash_value = hash_value & 0xFFFFFFFF  # 32ë¹„íŠ¸ ì •ìˆ˜ë¡œ ì œí•œ

        return str(hash_value)
    except Exception as e:
        print(f"âŒ ì²´í¬ì„¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return "0"

def compress_json_data(data):
    """
    JSON ë°ì´í„°ë¥¼ gzip ì••ì¶• í›„ Base64 ì¸ì½”ë”©
    Google Drive ì €ì¥ìš© ì••ì¶• í•¨ìˆ˜
    """
    try:
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        json_str = json.dumps(data, ensure_ascii=False, indent=2)

        # UTF-8ë¡œ ì¸ì½”ë”© í›„ gzip ì••ì¶•
        json_bytes = json_str.encode('utf-8')
        compressed_bytes = gzip.compress(json_bytes)

        # Base64 ì¸ì½”ë”©
        compressed_b64 = base64.b64encode(compressed_bytes).decode('ascii')

        # ì••ì¶• í†µê³„ ê³„ì‚°
        original_size = len(json_bytes)
        compressed_size = len(compressed_bytes)
        compression_ratio = (compressed_size / original_size * 100) if original_size > 0 else 100

        print(f"ğŸ“¦ ë°ì´í„° ì••ì¶• ì™„ë£Œ: {original_size}B â†’ {compressed_size}B ({compression_ratio:.1f}%)")

        return compressed_b64, original_size, compressed_size
    except Exception as e:
        print(f"âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
        raise e

def decompress_json_data(compressed_b64):
    """
    Base64 ì••ì¶• ë°ì´í„°ë¥¼ í•´ì œí•˜ì—¬ JSONìœ¼ë¡œ ë³µì›
    Google Drive ë¡œë“œìš© í•´ì œ í•¨ìˆ˜
    """
    try:
        # Base64 ë””ì½”ë”©
        compressed_bytes = base64.b64decode(compressed_b64)

        # gzip í•´ì œ
        decompressed_bytes = gzip.decompress(compressed_bytes)

        # JSON íŒŒì‹±
        json_str = decompressed_bytes.decode('utf-8')
        return json.loads(json_str)
    except Exception as e:
        print(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
        raise e

def create_metadata(schedules, timestamp, version, user_id, original_size, compressed_size, device_uuid=None):
    """ë©”íƒ€ë°ì´í„° ìƒì„±"""
    metadata = {
        "timestamp": timestamp.isoformat(),
        "version": version,
        "user_id": user_id,
        "checksum": calculate_checksum(schedules),
        "count": len(schedules),
        "original_size": original_size,
        "compressed_size": compressed_size
    }

    # ë””ë°”ì´ìŠ¤ UUIDê°€ ìˆìœ¼ë©´ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
    if device_uuid:
        metadata["device_uuid"] = device_uuid

    return metadata

def save_metadata_to_drive(access_token, folder_id, filename, metadata, log_func=print):
    """ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ Google Driveì— ì €ì¥"""
    try:
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ëª… ìƒì„±
        meta_filename = filename.replace('.json', '.meta.json')

        # íŒŒì¼ ë©”íƒ€ë°ì´í„°
        file_metadata = {
            'name': meta_filename,
            'parents': [folder_id] if folder_id else []
        }

        # ì—…ë¡œë“œ ì¤€ë¹„
        headers = {'Authorization': f'Bearer {access_token}'}

        files = {
            'metadata': (None, json.dumps(file_metadata), 'application/json'),
            'media': (meta_filename, json.dumps(metadata, ensure_ascii=False, indent=2), 'application/json')
        }

        response = requests.post(
            'https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart',
            headers=headers,
            files=files
        )

        if response.status_code == 200:
            result = response.json()
            log_func(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {result.get('id')}")
            return result.get('id')
        else:
            log_func(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return None

    except Exception as e:
        log_func(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        return None

def load_metadata_from_drive(service, folder_id, filename):
    """Google Driveì—ì„œ ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ"""
    try:
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²€ìƒ‰
        meta_filename = filename.replace('.json', '.meta.json')
        query = f"name='{meta_filename}' and '{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id, name)").execute()
        files = results.get('files', [])

        if not files:
            print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ: {meta_filename}")
            return None

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        file_id = files[0]['id']
        request = service.files().get_media(fileId=file_id)
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)

        done = False
        while done is False:
            status, done = downloader.next_chunk()

        file_io.seek(0)
        content = file_io.read().decode('utf-8')
        metadata = json.loads(content)

        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì²´í¬ì„¬={metadata.get('checksum', 'N/A')}, ê°œìˆ˜={metadata.get('count', 0)}")
        return metadata

    except Exception as e:
        print(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def save_json_compressed(data, file_path):
    """Save JSON data with gzip compression"""
    with gzip.open(f"{file_path}.gz", 'wt', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json_compressed(file_path):
    """Load JSON data from gzip compressed file"""
    with gzip.open(f"{file_path}.gz", 'rt', encoding='utf-8') as f:
        return json.load(f)

def save_json_regular(data, file_path):
    """Save JSON data without compression"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def get_file_size_mb(file_path):
    """Get file size in MB"""
    try:
        return os.path.getsize(file_path) / (1024 * 1024)
    except:
        return 0

# --- API Endpoints ---

@app.post("/auth/google")
async def google_auth(auth_request: GoogleAuthRequest, db: Session = Depends(get_database)):
    """Exchange Google authorization code for user info."""
    try:
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ redirect_uri ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        redirect_uri = auth_request.redirect_uri or GOOGLE_REDIRECT_URI

        print(f"ğŸ”‘ ë°›ì€ ì¸ì¦ ì½”ë“œ: {auth_request.code[:20]}...")
        print(f"ğŸ”— ì‚¬ìš©í•  redirect_uri: {redirect_uri}")

        # Step 1: Exchange authorization code for access token
        token_url = "https://oauth2.googleapis.com/token"
        token_data = {
            'client_id': GOOGLE_CLIENT_ID,
            'client_secret': GOOGLE_CLIENT_SECRET,
            'code': auth_request.code,
            'grant_type': 'authorization_code',
            'redirect_uri': redirect_uri
        }

        token_response = requests.post(token_url, data=token_data)

        print(f"ğŸ“¡ í† í° ì‘ë‹µ ìƒíƒœ: {token_response.status_code}")
        if not token_response.ok:
            print(f"âŒ í† í° ì—ëŸ¬: {token_response.text}")

        if not token_response.ok:
            raise HTTPException(status_code=400, detail=f"Token exchange failed: {token_response.text}")

        token_json = token_response.json()
        access_token = token_json.get('access_token')
        refresh_token = token_json.get('refresh_token')

        if not access_token:
            raise HTTPException(status_code=400, detail="No access token received")

        # Step 2: Get user info using access token
        user_info_url = f"https://www.googleapis.com/oauth2/v2/userinfo?access_token={access_token}"
        user_response = requests.get(user_info_url)

        if not user_response.ok:
            raise HTTPException(status_code=400, detail=f"User info fetch failed: {user_response.text}")

        user_data = user_response.json()

        # Save or update user in database
        user_id = f"google_{user_data.get('id')}"
        google_id = user_data.get('id')

        existing_user = db.query(User).filter(User.id == user_id).first()

        if existing_user:
            # Update existing user (is_admin ê°’ì€ ìœ ì§€)
            existing_user.email = user_data.get("email")
            existing_user.name = user_data.get("name")
            existing_user.last_login = func.now()
            admin_badge = "ğŸ”‘ [ê´€ë¦¬ì]" if existing_user.is_admin else ""
            print(f"âœ… ê¸°ì¡´ ì‚¬ìš©ì ë¡œê·¸ì¸: {existing_user.name} ({existing_user.email}) {admin_badge}")
        else:
            # Create new user (ì‹ ê·œ ì‚¬ìš©ìë§Œ DEV_ADMIN_ID ì²´í¬)
            is_admin = (google_id == DEV_ADMIN_ID) if DEV_ADMIN_ID else False
            new_user = User(
                id=user_id,
                auth_provider="google",
                is_anonymous=False,
                email=user_data.get("email"),
                name=user_data.get("name"),
                is_admin=is_admin
            )
            db.add(new_user)
            admin_badge = "ğŸ”‘ [ê´€ë¦¬ì]" if is_admin else ""
            print(f"ğŸ†• ì‹ ê·œ ì‚¬ìš©ì ìƒì„±: {new_user.name} ({new_user.email}) {admin_badge}")

        db.commit()

        # Return user information
        return {
            "id": user_data.get("id"),
            "name": user_data.get("name"),
            "email": user_data.get("email"),
            "picture": user_data.get("picture"),
            "is_admin": is_admin,
            "access_token": access_token,
            "refresh_token": refresh_token
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Authentication failed: {str(e)}")

@app.post("/auth/naver")
async def naver_auth(auth_request: NaverAuthRequest, db: Session = Depends(get_database)):
    """Exchange Naver authorization code for user info."""
    try:
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            raise HTTPException(status_code=500, detail="Naver OAuth is not configured")

        print(f"ğŸ”‘ ë„¤ì´ë²„ ì¸ì¦ ì½”ë“œ: {auth_request.code[:20]}...")
        print(f"ğŸ”‘ ë„¤ì´ë²„ state: {auth_request.state}")

        # Step 1: Exchange authorization code for access token
        token_url = "https://nid.naver.com/oauth2.0/token"
        token_params = {
            'grant_type': 'authorization_code',
            'client_id': NAVER_CLIENT_ID,
            'client_secret': NAVER_CLIENT_SECRET,
            'code': auth_request.code,
            'state': auth_request.state
        }

        token_response = requests.get(token_url, params=token_params)

        print(f"ğŸ“¡ í† í° ì‘ë‹µ ìƒíƒœ: {token_response.status_code}")
        if not token_response.ok:
            print(f"âŒ í† í° ì—ëŸ¬: {token_response.text}")
            raise HTTPException(status_code=400, detail=f"Token exchange failed: {token_response.text}")

        token_json = token_response.json()
        access_token = token_json.get('access_token')
        refresh_token = token_json.get('refresh_token')

        if not access_token:
            raise HTTPException(status_code=400, detail="No access token received")

        # Step 2: Get user info using access token
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {
            'Authorization': f'Bearer {access_token}'
        }
        user_response = requests.get(user_info_url, headers=headers)

        if not user_response.ok:
            raise HTTPException(status_code=400, detail=f"User info fetch failed: {user_response.text}")

        user_data = user_response.json()

        # ë„¤ì´ë²„ API ì‘ë‹µ êµ¬ì¡°: { "resultcode": "00", "message": "success", "response": { ... } }
        if user_data.get('resultcode') != '00':
            raise HTTPException(status_code=400, detail=f"Naver API error: {user_data.get('message')}")

        naver_user = user_data.get('response', {})
        naver_id = naver_user.get('id')

        # Save or update user in database
        user_id = f"naver_{naver_id}"

        existing_user = db.query(User).filter(User.id == user_id).first()

        if existing_user:
            # Update existing user
            existing_user.email = naver_user.get("email")
            existing_user.name = naver_user.get("name") or naver_user.get("nickname")
            existing_user.last_login = func.now()
            print(f"âœ… ê¸°ì¡´ ì‚¬ìš©ì ë¡œê·¸ì¸: {existing_user.name} ({existing_user.email})")
        else:
            # Create new user
            new_user = User(
                id=user_id,
                auth_provider="naver",
                is_anonymous=False,
                email=naver_user.get("email"),
                name=naver_user.get("name") or naver_user.get("nickname"),
                is_admin=False
            )
            db.add(new_user)
            print(f"ğŸ†• ì‹ ê·œ ì‚¬ìš©ì ìƒì„±: {new_user.name} ({new_user.email})")

        db.commit()

        # Get the final user object to return current is_admin value
        final_user = db.query(User).filter(User.id == user_id).first()

        # Return user information
        return {
            "id": naver_id,
            "name": naver_user.get("name") or naver_user.get("nickname"),
            "email": naver_user.get("email"),
            "picture": naver_user.get("profile_image"),
            "is_admin": final_user.is_admin if final_user else False,
            "access_token": access_token,
            "refresh_token": refresh_token
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Naver authentication failed: {str(e)}")

@app.post("/auth/kakao")
async def kakao_auth(auth_request: KakaoAuthRequest, db: Session = Depends(get_database)):
    """Exchange Kakao authorization code for user info."""
    try:
        if not KAKAO_REST_API_KEY:
            raise HTTPException(status_code=500, detail="Kakao OAuth is not configured")

        print(f"ğŸ”‘ ì¹´ì¹´ì˜¤ ì¸ì¦ ì½”ë“œ: {auth_request.code[:20]}...")

        # Step 1: Exchange authorization code for access token
        token_url = "https://kauth.kakao.com/oauth/token"
        token_data = {
            'grant_type': 'authorization_code',
            'client_id': KAKAO_REST_API_KEY,
            'redirect_uri': KAKAO_REDIRECT_URI,
            'code': auth_request.code
        }

        # Add client_secret if available
        if KAKAO_CLIENT_SECRET:
            token_data['client_secret'] = KAKAO_CLIENT_SECRET

        token_response = requests.post(token_url, data=token_data)

        print(f"ğŸ“¡ í† í° ì‘ë‹µ ìƒíƒœ: {token_response.status_code}")
        if not token_response.ok:
            print(f"âŒ í† í° ì—ëŸ¬: {token_response.text}")
            raise HTTPException(status_code=400, detail=f"Token exchange failed: {token_response.text}")

        token_json = token_response.json()
        access_token = token_json.get('access_token')
        refresh_token = token_json.get('refresh_token')

        if not access_token:
            raise HTTPException(status_code=400, detail="No access token received")

        # Step 2: Get user info using access token
        user_info_url = "https://kapi.kakao.com/v2/user/me"
        headers = {
            'Authorization': f'Bearer {access_token}'
        }
        user_response = requests.get(user_info_url, headers=headers)

        if not user_response.ok:
            raise HTTPException(status_code=400, detail=f"User info fetch failed: {user_response.text}")

        user_data = user_response.json()
        kakao_account = user_data.get('kakao_account', {})
        profile = kakao_account.get('profile', {})

        kakao_id = str(user_data.get('id'))  # ìˆ«ìë¡œ ì˜¤ë¯€ë¡œ ë¬¸ìì—´ ë³€í™˜

        # Save or update user in database
        user_id = f"kakao_{kakao_id}"

        existing_user = db.query(User).filter(User.id == user_id).first()

        if existing_user:
            # Update existing user
            existing_user.email = kakao_account.get("email")
            existing_user.name = profile.get("nickname")
            existing_user.last_login = func.now()
            print(f"âœ… ê¸°ì¡´ ì‚¬ìš©ì ë¡œê·¸ì¸: {existing_user.name} ({existing_user.email})")
        else:
            # Create new user
            new_user = User(
                id=user_id,
                auth_provider="kakao",
                is_anonymous=False,
                email=kakao_account.get("email"),
                name=profile.get("nickname"),
                is_admin=False
            )
            db.add(new_user)
            print(f"ğŸ†• ì‹ ê·œ ì‚¬ìš©ì ìƒì„±: {new_user.name} ({new_user.email})")

        db.commit()

        # Get the final user object to return current is_admin value
        final_user = db.query(User).filter(User.id == user_id).first()

        # Return user information
        return {
            "id": kakao_id,
            "name": profile.get("nickname"),
            "email": kakao_account.get("email"),
            "picture": profile.get("profile_image_url"),
            "is_admin": final_user.is_admin if final_user else False,
            "access_token": access_token,
            "refresh_token": refresh_token
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Kakao authentication failed: {str(e)}")

@app.post("/auth/refresh")
async def refresh_token(request: dict = Body(...)):
    """Refresh Google OAuth token"""
    try:
        refresh_token = request.get('refresh_token')
        if not refresh_token:
            raise HTTPException(status_code=400, detail="Refresh token is required")

        # Exchange refresh token for new access token
        token_response = requests.post(
            'https://oauth2.googleapis.com/token',
            data={
                'client_id': GOOGLE_CLIENT_ID,
                'client_secret': GOOGLE_CLIENT_SECRET,
                'refresh_token': refresh_token,
                'grant_type': 'refresh_token'
            }
        )

        if not token_response.ok:
            raise HTTPException(status_code=400, detail=f"Token refresh failed: {token_response.text}")

        token_json = token_response.json()
        new_access_token = token_json.get('access_token')
        new_refresh_token = token_json.get('refresh_token', refresh_token)  # ìƒˆ refresh tokenì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ê²ƒ ì‚¬ìš©

        if not new_access_token:
            raise HTTPException(status_code=400, detail="No access token received")

        return {
            "access_token": new_access_token,
            "refresh_token": new_refresh_token,
            "success": True
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Token refresh failed: {str(e)}")

# --- User Management API ---
@app.get("/api/users")
async def get_users(db: Session = Depends(get_database)):
    """Get all users (for admin panel)"""
    try:
        users = db.query(User).order_by(User.created_at.desc()).all()
        return [user.to_dict() for user in users]
    except Exception as e:
        logger.error(f"âŒ Failed to get users: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get users: {str(e)}")

@app.get("/api/parse-file")
def parse_from_file():
    """Reads the predefined data file, parses it, and returns the schedules."""
    try:
        with open(DATA_FILE_PATH, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        return {"data": parse_schedules(raw_content), "success": True}
    except FileNotFoundError:
        return {"error": f"Data file not found at {DATA_FILE_PATH}", "success": False}
    except Exception as e:
        return {"error": f"An error occurred: {str(e)}", "success": False}

@app.post("/api/parse-text")
def parse_from_text(request: ParseTextRequest):
    """Receives raw text and engine selection, parses it, and returns the schedules."""
    try:
        text = request.text
        engine = request.engine

        # Select parser based on engine parameter
        print(f"ğŸ”§ Using engine: {engine}")
        if engine == "classic":
            print("ğŸ“œ Running classic-only parser...")
            data = parse_schedules_classic_only(text)
            print(f"ğŸ“œ Classic parser result: {len(data)} schedules")
        elif engine == "ai_only":
            print("ğŸ¤– Running AI-only parser...")
            data = parse_schedules_ai_only(text)
            print(f"ğŸ¤– AI parser result: {len(data)} schedules")
        elif engine == "hybrid":
            print("ğŸ”€ Running hybrid parser...")
            data = parse_schedules(text)
            print(f"ğŸ”€ Hybrid parser result: {len(data)} schedules")
        else:
            return {"error": f"Unknown parser engine: {engine}", "success": False}

        return {"data": data, "success": True, "engine_used": engine}
    except Exception as e:
        return {"error": f"An error occurred during parsing: {str(e)}", "success": False}

@app.post("/api/parse-uploaded-file")
async def parse_uploaded_file(file: UploadFile = File(...), engine: str = "classic"):
    """Receives an uploaded file, parses it, and returns the schedules."""
    try:
        # Check file type
        if not file.filename.endswith('.txt'):
            return {"error": "Only .txt files are supported", "success": False}

        # Read file content
        content = await file.read()
        raw_content = content.decode('utf-8')

        # Select parser based on engine parameter
        print(f"ğŸ”§ File upload using engine: {engine}")
        if engine == "classic":
            print("ğŸ“œ Running classic-only parser on uploaded file...")
            data = parse_schedules_classic_only(raw_content)
            print(f"ğŸ“œ Classic parser result: {len(data)} schedules")
        elif engine == "ai_only":
            print("ğŸ¤– Running AI-only parser on uploaded file...")
            data = parse_schedules_ai_only(raw_content)
            print(f"ğŸ¤– AI parser result: {len(data)} schedules")
        elif engine == "hybrid":
            print("ğŸ”€ Running hybrid parser on uploaded file...")
            data = parse_schedules(raw_content)
            print(f"ğŸ”€ Hybrid parser result: {len(data)} schedules")
        else:
            return {"error": f"Unknown parser engine: {engine}", "success": False}

        return {"data": data, "success": True, "engine_used": engine}
    except Exception as e:
        return {"error": f"An error occurred during file parsing: {str(e)}", "success": False}

@app.get("/api/get-raw-data")
def get_raw_data():
    """Returns the raw content of the data file for frontend processing."""
    try:
        with open(DATA_FILE_PATH, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        return {"data": raw_content, "success": True}
    except FileNotFoundError:
        return {"error": f"Data file not found at {DATA_FILE_PATH}", "success": False}
    except Exception as e:
        return {"error": f"An error occurred: {str(e)}", "success": False}

@app.post("/api/save-schedules")
def save_schedules_to_server(request: SaveSchedulesRequest):
    """Save schedules to Google Drive and server with user-specific storage."""

    # ë¡œê·¸ ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    debug_logs = []

    def log_and_collect(message):
        print(message)
        debug_logs.append(message)

    try:
        # Persistent storageì— ì €ì¥í•  ë°ì´í„°
        schedules_data = request.schedules_data

        # ë°ì´í„° í¬ê¸° í™•ì¸
        if isinstance(schedules_data, dict) and 'schedules' in schedules_data:
            schedules_count = len(schedules_data['schedules'])
        else:
            schedules_count = 0

        log_and_collect(f"ğŸ“¥ Persistent Save request: user_id={request.user_id}, schedules_count={schedules_count}")
        log_and_collect(f"ğŸ” Request validation successful. Processing persistent storage...")

        log_and_collect(f"ğŸ“Š ì €ì¥í•  ìŠ¤ì¼€ì¤„ ê°œìˆ˜: {schedules_count}")
        user_id = request.user_id

        # Save to persistent storage
        success = save_to_persistent_storage(user_id, schedules_data)

        if success:
            return {
                "success": True,
                "message": f"Persistent storage save successful for user {user_id}",
                "saved_schedules": schedules_count,
                "storage_type": "persistent_volume"
            }
        else:
            return {
                "success": False,
                "error": "Failed to save to persistent storage"
            }
    except Exception as e:
        return {"error": f"Failed to save schedules: {str(e)}", "success": False}

@app.post("/api/check-sync-metadata")
def check_sync_metadata(request: LoadSchedulesRequest):
    """í´ë¼ìš°ë“œ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸í•˜ì—¬ ë™ê¸°í™” í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    try:
        user_id = request.user_id
        access_token = request.access_token
        refresh_token = request.refresh_token

        if not user_id:
            return {"success": False, "message": "No user ID provided"}

        if not access_token:
            return {"success": False, "message": "No access token provided"}

        try:
            service = get_drive_service(access_token, refresh_token)
            folder_id = find_or_create_app_folder(service)
            filename = f"schedules_{user_id}.json"

            # ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ
            metadata = load_metadata_from_drive(service, folder_id, filename)

            if metadata:
                return {
                    "success": True,
                    "has_cloud_data": True,
                    "metadata": metadata,
                    "message": "í´ë¼ìš°ë“œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì„±ê³µ"
                }
            else:
                return {
                    "success": True,
                    "has_cloud_data": False,
                    "metadata": None,
                    "message": "í´ë¼ìš°ë“œì— ë°ì´í„° ì—†ìŒ"
                }

        except Exception as e:
            return {
                "success": False,
                "message": f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
            }

    except Exception as e:
        return {
            "success": False,
            "message": f"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        }

@app.post("/api/load-schedules")
def load_schedules_from_server(request: LoadSchedulesRequest):
    """Load latest schedules from Google Drive with local fallback."""
    try:
        user_id = request.user_id
        access_token = request.access_token
        refresh_token = request.refresh_token

        if not user_id:
            return {"data": [], "success": True, "message": "No user ID provided"}

        drive_data = None
        local_data = None

        # Try to load from Google Drive first
        if access_token:
            try:
                service = get_drive_service(access_token, refresh_token)
                folder_id = find_or_create_app_folder(service)
                filename = f"schedules_{user_id}.json"
                drive_data = load_from_drive(service, folder_id, filename)
                if drive_data:
                    print(f"âœ… Google Drive load successful")
                else:
                    print(f"ğŸ“ No data found in Google Drive")

            except Exception as drive_error:
                print(f"âŒ Google Drive load failed: {drive_error}")

        # Load from local storage as fallback
        user_data_dir = os.path.join(SCHEDULES_DATA_DIR, "users", user_id)
        latest_path = os.path.join(user_data_dir, "schedules_latest.json")

        if os.path.exists(latest_path):
            with open(latest_path, 'r', encoding='utf-8') as f:
                local_data = json.load(f)
            print(f"ğŸ“± Local data loaded")

        # Determine which data to use (prefer newer timestamp)
        final_data = None
        data_source = "none"

        if drive_data and local_data:
            drive_timestamp = drive_data.get("timestamp", "")
            local_timestamp = local_data.get("timestamp", "")
            if drive_timestamp >= local_timestamp:
                final_data = drive_data
                data_source = "drive"
            else:
                final_data = local_data
                data_source = "local"
        elif drive_data:
            final_data = drive_data
            data_source = "drive"
        elif local_data:
            final_data = local_data
            data_source = "local"

        if final_data:
            return {
                "data": final_data,
                "success": True,
                "source": data_source,
                "message": f"Schedules loaded from {data_source}"
            }
        else:
            return {
                "data": [],
                "success": True,
                "source": "none",
                "message": f"No saved schedules found for user {user_id}"
            }

    except Exception as e:
        return {"error": f"Failed to load schedules: {str(e)}", "success": False}

# Keep the old GET endpoint for backward compatibility
@app.get("/api/load-schedules")
def load_schedules_from_server_get(user_id: str = Query(None)):
    """Load latest schedules from local server only (backward compatibility)."""
    try:
        if not user_id:
            return {"data": [], "success": True, "message": "No user ID provided"}

        # Load user-specific schedules from local only
        user_data_dir = os.path.join(SCHEDULES_DATA_DIR, "users", user_id)
        latest_path = os.path.join(user_data_dir, "schedules_latest.json")

        if not os.path.exists(latest_path):
            return {"data": [], "success": True, "message": f"No saved schedules found for user {user_id}"}

        with open(latest_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        return {
            "data": data.get("schedules", []),
            "success": True,
            "version": data.get("version"),
            "timestamp": data.get("timestamp"),
            "user_id": user_id,
            "source": "local"
        }
    except Exception as e:
        return {"error": f"Failed to load schedules: {str(e)}", "success": False}


# === DATABASE BACKUP/RESTORE APIs ===

@app.get("/api/backup-database")
async def backup_database(user_id: str = Query(...)):
    """ì‚¬ìš©ìë³„ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ì•ˆì „í•˜ê²Œ ë°±ì—…"""
    db = None
    try:
        logger.info(f"ğŸ”„ Starting backup for user: {user_id}")

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        db = SessionLocal()
        logger.info("âœ… Database connection established")

        service = ScheduleService(db)
        logger.info("âœ… ScheduleService initialized")

        # Google ì‚¬ìš©ì IDì— ì ‘ë‘ì‚¬ ì¶”ê°€
        original_user_id = user_id
        if not user_id.startswith('google_') and user_id != 'anonymous':
            user_id = f'google_{user_id}'

        logger.info(f"ğŸ”„ Processed user ID: {original_user_id} -> {user_id}")

        # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ìŠ¤ì¼€ì¤„ ê°€ì ¸ì˜¤ê¸°
        logger.info(f"ğŸ”„ Fetching schedules for user: {user_id}")
        schedules = service.get_all_schedules(user_id)
        logger.info(f"âœ… Found {len(schedules) if schedules else 0} schedules")

        if not schedules:
            logger.warning(f"âš ï¸  No schedules found for user: {user_id}")
            return {
                "success": False,
                "message": "ë°±ì—…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # JSON í˜•íƒœë¡œ ë³€í™˜ (ID ì œì™¸, ì‚¬ìš©ìë³„ ë°ì´í„°ë§Œ)
        logger.info("ğŸ”„ Converting schedules to dict format")

        # schedulesë¥¼ ì•ˆì „í•˜ê²Œ dictë¡œ ë³€í™˜
        schedule_dicts = []
        for i, schedule in enumerate(schedules):
            try:
                schedule_dict = schedule.to_dict()
                schedule_dicts.append(schedule_dict)
                logger.debug(f"âœ… Converted schedule {i+1}/{len(schedules)}")
            except Exception as e:
                logger.error(f"âŒ Failed to convert schedule {i+1}: {str(e)}")
                # ë³€í™˜ ì‹¤íŒ¨í•œ ìŠ¤ì¼€ì¤„ì€ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                continue

        backup_data = {
            "version": "v2025.01",
            "backup_date": datetime.now().isoformat(),
            "user_id": user_id,
            "schedules": schedule_dicts
        }

        logger.info(f"âœ… Backup data prepared with {len(schedule_dicts)} schedules")

        return {
            "success": True,
            "backup_data": backup_data,
            "count": len(schedule_dicts),
            "message": f"{len(schedule_dicts)}ê°œì˜ ìŠ¤ì¼€ì¤„ì´ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        error_msg = f"ë°±ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        logger.error(f"âŒ Backup failed: {str(e)}")
        logger.error(f"âŒ Error type: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ Traceback: {traceback.format_exc()}")

        return {
            "success": False,
            "message": error_msg
        }
    finally:
        if db:
            try:
                db.close()
                logger.info("âœ… Database connection closed")
            except Exception as e:
                logger.error(f"âŒ Error closing database: {str(e)}")


@app.post("/api/restore-database")
async def restore_database(request: dict = Body(...)):
    """ì‚¬ìš©ìë³„ JSON ë°±ì—… ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë³µì›"""
    try:
        db = SessionLocal()
        service = ScheduleService(db)

        user_id = request.get('user_id')
        backup_data = request.get('backup_data')

        # Google ì‚¬ìš©ì IDì— ì ‘ë‘ì‚¬ ì¶”ê°€
        if user_id and not user_id.startswith('google_') and user_id != 'anonymous':
            user_id = f'google_{user_id}'

        if not user_id or not backup_data:
            return {
                "success": False,
                "message": "ì‚¬ìš©ì IDì™€ ë°±ì—… ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }

        # ë°±ì—… ë°ì´í„° ê²€ì¦
        if not isinstance(backup_data, dict) or 'schedules' not in backup_data:
            return {
                "success": False,
                "message": "ìœ íš¨í•˜ì§€ ì•Šì€ ë°±ì—… ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤."
            }

        schedules_data = backup_data['schedules']
        if not isinstance(schedules_data, list):
            return {
                "success": False,
                "message": "ìŠ¤ì¼€ì¤„ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }

        # í•´ë‹¹ ì‚¬ìš©ìì˜ ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ
        existing_count = service.get_schedule_count(user_id)
        if existing_count > 0:
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            from database import Schedule
            db.query(Schedule).filter(Schedule.user_id == user_id).delete()
            db.commit()

        # ìƒˆë¡œìš´ ë°ì´í„° ë²Œí¬ ì¶”ê°€ (ì„±ëŠ¥ ìµœì í™”)
        added_count = 0
        schedules_to_insert = []

        for schedule_data in schedules_data:
            try:
                # ID í•„ë“œ ì œê±° (ìƒˆë¡œ ìƒì„±ë˜ë„ë¡)
                if 'id' in schedule_data:
                    del schedule_data['id']

                # ìŠ¤ì¼€ì¤„ ê°ì²´ ìƒì„± (ì•„ì§ DBì— ì‚½ì…í•˜ì§€ ì•ŠìŒ)
                from database import Schedule
                schedule = Schedule.from_dict(schedule_data, user_id)
                schedules_to_insert.append(schedule)
                added_count += 1
            except Exception as e:
                print(f"ìŠ¤ì¼€ì¤„ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
                continue

        # ë²Œí¬ ì‚½ì… (í•œ ë²ˆì— ëª¨ë“  ë°ì´í„° ì‚½ì…)
        if schedules_to_insert:
            db.add_all(schedules_to_insert)
            db.commit()

        return {
            "success": True,
            "message": f"ë³µì› ì™„ë£Œ: {existing_count}ê°œ ì‚­ì œ, {added_count}ê°œ ì¶”ê°€",
            "deleted_count": existing_count,
            "added_count": added_count
        }

    except Exception as e:
        return {
            "success": False,
            "message": f"ë³µì› ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }
    finally:
        db.close()


def cleanup_old_backups():
    """Clean up backup files older than 7 days"""
    try:
        users_dir = os.path.join(SCHEDULES_DATA_DIR, "users")
        if not os.path.exists(users_dir):
            return

        cutoff_time = datetime.now() - timedelta(days=7)
        cleaned_count = 0

        for user_folder in os.listdir(users_dir):
            user_path = os.path.join(users_dir, user_folder)
            if not os.path.isdir(user_path):
                continue

            for filename in os.listdir(user_path):
                file_path = os.path.join(user_path, filename)

                # Skip latest files and settings
                if filename in ["schedules_latest.json", "app_settings.json"]:
                    continue

                # Check if file is backup file (schedules_*.json.gz)
                if filename.startswith('schedules_') and filename.endswith('.json.gz'):
                    if os.path.isfile(file_path):
                        file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                        if file_time < cutoff_time:
                            file_size = get_file_size_mb(file_path)
                            os.remove(file_path)
                            cleaned_count += 1
                            print(f"Cleaned up: {filename} ({file_size:.2f}MB)")

        return cleaned_count
    except Exception as e:
        print(f"Cleanup error: {e}")
        return 0

@app.post("/api/cleanup-backups")
def manual_cleanup():
    """Manually trigger backup cleanup"""
    try:
        cleaned_count = cleanup_old_backups()
        return {
            "success": True,
            "message": f"Cleaned up {cleaned_count} old backup files",
            "cleaned_count": cleaned_count
        }
    except Exception as e:
        return {"error": f"Cleanup failed: {str(e)}", "success": False}

# --- Railway Persistent Storage API Endpoints ---
@app.post("/api/persistent/save")
async def save_to_database(request: PersistentSaveRequest, db: Session = Depends(get_database)):
    """Save schedules to PostgreSQL database"""
    try:
        user_id = request.user_id
        schedules_data = request.schedules_data

        # Extract schedules array from schedules_data
        schedules = []
        if isinstance(schedules_data, dict) and 'schedules' in schedules_data:
            schedules = schedules_data['schedules']
        elif isinstance(schedules_data, list):
            schedules = schedules_data

        schedules_count = len(schedules)
        print(f"ğŸ“¥ Database save request: schedules_count={schedules_count}")

        # Save to database using ScheduleService
        service = ScheduleService(db)
        saved_schedules = service.save_schedules(user_id, schedules)

        # Auto-create tags from saved schedules
        for schedule in schedules:
            brand = schedule.get('brand', '')
            album = schedule.get('album', '')
            if brand or album:
                auto_create_tags_from_schedule(db, user_id, brand, album)

        db.commit()

        return {
            "success": True,
            "message": f"Successfully saved {schedules_count} schedules to database",
            "schedules_count": schedules_count,
            "storage_type": "postgresql"
        }

    except Exception as e:
        print(f"âŒ Database save error: {e}")
        raise HTTPException(status_code=500, detail=f"Database save failed: {str(e)}")

@app.post("/api/persistent/load")
async def load_from_database(request: PersistentLoadRequest, db: Session = Depends(get_database)):
    """Load schedules from PostgreSQL database"""
    try:
        user_id = request.user_id

        print(f"ğŸ“¤ Database load request")

        # Load from database using ScheduleService
        service = ScheduleService(db)
        schedules = service.get_schedules(user_id)

        # Convert to dictionaries (compatible with frontend)
        schedules_data = [schedule.to_dict() for schedule in schedules]

        return {
            "success": True,
            "data": schedules_data,
            "last_modified": datetime.now().isoformat(),
            "source": "postgresql",
            "message": f"Successfully loaded {len(schedules_data)} schedules from database"
        }

    except Exception as e:
        print(f"âŒ Database load error: {e}")
        raise HTTPException(status_code=500, detail=f"Database load failed: {str(e)}")

@app.get("/api/persistent/status/{user_id}")
async def get_database_status_api(user_id: str, db: Session = Depends(get_database)):
    """Get database storage status for a user"""
    try:
        print(f"ğŸ“Š Database status request")

        # Check database for user data
        service = ScheduleService(db)
        schedules_count = service.get_schedule_count(user_id)
        has_data = schedules_count > 0

        status = {
            "user_id": user_id,
            "has_data": has_data,
            "schedules_count": schedules_count,
            "storage_type": "postgresql"
        }

        return {
            "success": True,
            **status
        }

    except Exception as e:
        print(f"âŒ Database status error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get database status: {str(e)}")

# --- New Database-Specific API Endpoints ---

@app.put("/api/schedules/{schedule_id}/field/{field}")
async def update_schedule_field(
    schedule_id: int,
    field: str,
    value: dict = Body(...),  # {"value": "actual_value", "user_id": "user123"}
    db: Session = Depends(get_database)
):
    """Update a single field of a schedule"""
    try:
        user_id = value.get("user_id")
        field_value = value.get("value")

        if not user_id:
            raise HTTPException(status_code=400, detail="user_id is required")

        print(f"ğŸ”„ Update field request: schedule_id={schedule_id}, field={field}")

        service = ScheduleService(db)
        updated_schedule = service.update_schedule_field(user_id, schedule_id, field, field_value)

        if not updated_schedule:
            raise HTTPException(status_code=404, detail="Schedule not found")

        # Auto-create tag when brand or album field is updated
        if field in ['brand', 'album']:
            brand = updated_schedule.brand if field == 'brand' else ''
            album = updated_schedule.album if field == 'album' else ''
            if brand or album:
                auto_create_tags_from_schedule(db, user_id, brand, album)
                db.commit()

        return {
            "success": True,
            "message": f"Successfully updated {field}",
            "schedule": updated_schedule.to_dict()
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"âŒ Update field error: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")

@app.delete("/api/schedules/{schedule_id}")
async def delete_schedule(
    schedule_id: int,
    user_id: str = Query(...),
    db: Session = Depends(get_database)
):
    """Delete a specific schedule"""
    try:
        print(f"ğŸ—‘ï¸ Delete schedule request: schedule_id={schedule_id}")

        service = ScheduleService(db)
        success = service.delete_schedule(user_id, schedule_id)

        if not success:
            raise HTTPException(status_code=404, detail="Schedule not found")

        return {
            "success": True,
            "message": f"Successfully deleted schedule {schedule_id}"
        }

    except Exception as e:
        print(f"âŒ Delete schedule error: {e}")
        raise HTTPException(status_code=500, detail=f"Delete failed: {str(e)}")

# ==================== User Management API ====================

@app.post("/api/users/init")
async def init_user(request: Request):
    """ì‚¬ìš©ì ì´ˆê¸°í™” ë˜ëŠ” ë¡œê·¸ì¸ ì‹œ í˜¸ì¶œ"""
    try:
        data = await request.json()
        user_id = data.get("user_id")
        auth_provider = data.get("auth_provider", "anonymous")
        is_anonymous = data.get("is_anonymous", False)
        email = data.get("email")
        name = data.get("name")

        if not user_id:
            raise HTTPException(status_code=400, detail="user_id is required")

        db_session = next(get_database())

        # ì‚¬ìš©ì ì¡°íšŒ ë˜ëŠ” ìƒì„±
        from database import User
        user = db_session.query(User).filter(User.id == user_id).first()

        if not user:
            # ì‹ ê·œ ì‚¬ìš©ì ìƒì„±
            user = User(
                id=user_id,
                auth_provider=auth_provider,
                is_anonymous=is_anonymous,
                email=email,
                name=name,
                has_seen_sample_data=False
            )
            db_session.add(user)
            db_session.commit()
            db_session.refresh(user)
            logger.info(f"âœ… New user created: {user_id} ({name or 'anonymous'})")
        else:
            # ê¸°ì¡´ ì‚¬ìš©ì - last_login ë° í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            user.last_login = func.now()
            # í”„ë¡œí•„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ (êµ¬ê¸€ ë¡œê·¸ì¸ ì‹œ ìµœì‹  ì •ë³´ ë°˜ì˜)
            if email:
                user.email = email
            if name:
                user.name = name
            db_session.commit()
            logger.info(f"âœ… User logged in: {user_id} ({name or 'anonymous'})")

        return {"success": True, "user": user.to_dict()}

    except Exception as e:
        logger.error(f"âŒ User init error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/users/{user_id}")
async def get_user(user_id: str):
    """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
    try:
        db_session = next(get_database())
        from database import User
        user = db_session.query(User).filter(User.id == user_id).first()

        if not user:
            raise HTTPException(status_code=404, detail="User not found")

        return {"success": True, "user": user.to_dict()}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Get user error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/users/{user_id}/sample-data")
async def mark_sample_data_seen(user_id: str):
    """ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë³¸ ê²ƒìœ¼ë¡œ í‘œì‹œ"""
    try:
        db_session = next(get_database())
        from database import User
        user = db_session.query(User).filter(User.id == user_id).first()

        if not user:
            raise HTTPException(status_code=404, detail="User not found")

        user.has_seen_sample_data = True
        db_session.commit()

        return {"success": True, "user": user.to_dict()}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Mark sample data seen error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/users")
async def list_users():
    """ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ (ê´€ë¦¬ììš©)"""
    try:
        db_session = next(get_database())
        from database import User, Schedule

        # ëª¨ë“  ì‚¬ìš©ì ì¡°íšŒ
        users = db_session.query(User).order_by(User.last_login.desc()).all()

        # ê° ì‚¬ìš©ìì˜ ìŠ¤ì¼€ì¤„ ê°œìˆ˜ ì¡°íšŒ
        user_list = []
        for user in users:
            schedule_count = db_session.query(Schedule).filter(Schedule.user_id == user.id).count()
            user_data = user.to_dict()
            user_data['schedule_count'] = schedule_count
            user_list.append(user_data)

        return {"success": True, "users": user_list, "total": len(user_list)}

    except Exception as e:
        logger.error(f"âŒ List users error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    return {"message": "Hello from the Schedule Parser backend!"}

@app.get("/privacy-policy", response_class=HTMLResponse)
def privacy_policy():
    """ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ í˜ì´ì§€"""
    return """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ - ë³¸ì‹ìŠ¤ëƒ…ëŸ¬</title>
        <style>
            body { font-family: 'Segoe UI', system-ui, sans-serif; line-height: 1.6; margin: 0; padding: 2rem; background: #f8f9fa; }
            .container { max-width: 800px; margin: 0 auto; background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #0d6efd; border-bottom: 2px solid #0d6efd; padding-bottom: 0.5rem; }
            h2 { color: #495057; margin-top: 2rem; }
            .date { color: #6c757d; margin-bottom: 2rem; }
            ul { padding-left: 1.5rem; }
            li { margin-bottom: 0.5rem; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨</h1>
            <p class="date">ìµœì¢… ìˆ˜ì •ì¼: 2024ë…„ 9ì›” 23ì¼</p>

            <h2>1. ê°œì¸ì •ë³´ ìˆ˜ì§‘ ë° ì´ìš© ëª©ì </h2>
            <p>ë³¸ì‹ìŠ¤ëƒ…ëŸ¬(ì´í•˜ "ì„œë¹„ìŠ¤")ëŠ” ë‹¤ìŒì˜ ëª©ì ì„ ìœ„í•´ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘ ë° ì´ìš©í•©ë‹ˆë‹¤:</p>
            <ul>
                <li>ìŠ¤ì¼€ì¤„ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì œê³µ</li>
                <li>ì‚¬ìš©ì ì¸ì¦ ë° ê³„ì • ê´€ë¦¬</li>
                <li>ë°ì´í„° ë™ê¸°í™” ë° ë°±ì—…</li>
                <li>ì„œë¹„ìŠ¤ ê°œì„  ë° ì‚¬ìš©ì ì§€ì›</li>
            </ul>

            <h2>2. ìˆ˜ì§‘í•˜ëŠ” ê°œì¸ì •ë³´ í•­ëª©</h2>
            <p>ì„œë¹„ìŠ¤ëŠ” ë‹¤ìŒì˜ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤:</p>
            <ul>
                <li><strong>Google ê³„ì • ì •ë³´:</strong> ì´ë¦„, ì´ë©”ì¼ ì£¼ì†Œ, í”„ë¡œí•„ ì‚¬ì§„</li>
                <li><strong>ì„œë¹„ìŠ¤ ì´ìš© ê¸°ë¡:</strong> ìŠ¤ì¼€ì¤„ ë°ì´í„°, ì„¤ì • ì •ë³´</li>
                <li><strong>ê¸°ìˆ ì  ì •ë³´:</strong> IP ì£¼ì†Œ, ë¸Œë¼ìš°ì € ì •ë³´, ì ‘ì† ë¡œê·¸</li>
            </ul>

            <h2>3. ê°œì¸ì •ë³´ ë³´ìœ  ë° ì´ìš© ê¸°ê°„</h2>
            <p>ìˆ˜ì§‘ëœ ê°œì¸ì •ë³´ëŠ” ë‹¤ìŒ ê¸°ê°„ ë™ì•ˆ ë³´ìœ ë©ë‹ˆë‹¤:</p>
            <ul>
                <li>íšŒì› íƒˆí‡´ ì‹œê¹Œì§€ (ì„œë¹„ìŠ¤ ì´ìš© ê¸°ê°„)</li>
                <li>ê´€ë ¨ ë²•ë ¹ì— ë”°ë¥¸ ë³´ì¡´ ì˜ë¬´ ê¸°ê°„</li>
                <li>ì‚¬ìš©ìê°€ ì§ì ‘ ì‚­ì œ ìš”ì²­ ì‹œ ì¦‰ì‹œ ì‚­ì œ</li>
            </ul>

            <h2>4. ê°œì¸ì •ë³´ ì œ3ì ì œê³µ</h2>
            <p>ì„œë¹„ìŠ¤ëŠ” ì‚¬ìš©ìì˜ ê°œì¸ì •ë³´ë¥¼ ì›ì¹™ì ìœ¼ë¡œ ì œ3ìì—ê²Œ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë‹¤ìŒì˜ ê²½ìš°ëŠ” ì˜ˆì™¸ë¡œ í•©ë‹ˆë‹¤:</p>
            <ul>
                <li>ì‚¬ìš©ìê°€ ì‚¬ì „ì— ë™ì˜í•œ ê²½ìš°</li>
                <li>ë²•ë ¹ì˜ ê·œì •ì— ì˜ê±°í•˜ê±°ë‚˜, ìˆ˜ì‚¬ ëª©ì ìœ¼ë¡œ ë²•ë ¹ì— ì •í•´ì§„ ì ˆì°¨ì™€ ë°©ë²•ì— ë”°ë¼ ìˆ˜ì‚¬ê¸°ê´€ì˜ ìš”êµ¬ê°€ ìˆëŠ” ê²½ìš°</li>
            </ul>

            <h2>5. ê°œì¸ì •ë³´ ì²˜ë¦¬ ìœ„íƒ</h2>
            <p>ì„œë¹„ìŠ¤ëŠ” ì›í™œí•œ ì„œë¹„ìŠ¤ ì œê³µì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ê°œì¸ì •ë³´ ì²˜ë¦¬ì—…ë¬´ë¥¼ ìœ„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤:</p>
            <ul>
                <li><strong>Google LLC:</strong> ì‚¬ìš©ì ì¸ì¦ ë° í´ë¼ìš°ë“œ ì €ì¥</li>
                <li><strong>Railway:</strong> ì„œë²„ í˜¸ìŠ¤íŒ… ë° ë°ì´í„° ì €ì¥</li>
            </ul>

            <h2>6. ì‚¬ìš©ìì˜ ê¶Œë¦¬</h2>
            <p>ì‚¬ìš©ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¶Œë¦¬ë¥¼ ê°€ì§‘ë‹ˆë‹¤:</p>
            <ul>
                <li>ê°œì¸ì •ë³´ ì²˜ë¦¬ í˜„í™©ì— ëŒ€í•œ ì—´ëŒ ìš”êµ¬</li>
                <li>ê°œì¸ì •ë³´ ìˆ˜ì •Â·ì‚­ì œ ìš”êµ¬</li>
                <li>ê°œì¸ì •ë³´ ì²˜ë¦¬ ì •ì§€ ìš”êµ¬</li>
                <li>ì†í•´ ë°œìƒ ì‹œ ì†í•´ë°°ìƒ ìš”êµ¬</li>
            </ul>

            <h2>7. ê°œì¸ì •ë³´ ë³´í˜¸ì±…ì„ì</h2>
            <p>ê°œì¸ì •ë³´ ì²˜ë¦¬ì— ê´€í•œ ì—…ë¬´ë¥¼ ì´ê´„í•´ì„œ ì±…ì„ì§€ê³ , ê°œì¸ì •ë³´ ì²˜ë¦¬ì™€ ê´€ë ¨í•œ ì •ë³´ì£¼ì²´ì˜ ë¶ˆë§Œì²˜ë¦¬ ë° í”¼í•´êµ¬ì œë¥¼ ìœ„í•˜ì—¬ ê°œì¸ì •ë³´ ë³´í˜¸ì±…ì„ìë¥¼ ì§€ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

            <h2>8. ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ ë³€ê²½</h2>
            <p>ì´ ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ì€ ì‹œí–‰ì¼ë¡œë¶€í„° ì ìš©ë˜ë©°, ë²•ë ¹ ë° ë°©ì¹¨ì— ë”°ë¥¸ ë³€ê²½ë‚´ìš©ì˜ ì¶”ê°€, ì‚­ì œ ë° ì •ì •ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ë³€ê²½ì‚¬í•­ì˜ ì‹œí–‰ 7ì¼ ì „ë¶€í„° ê³µì§€ì‚¬í•­ì„ í†µí•˜ì—¬ ê³ ì§€í•  ê²ƒì…ë‹ˆë‹¤.</p>
        </div>
    </body>
    </html>
    """

@app.get("/terms-of-service", response_class=HTMLResponse)
def terms_of_service():
    """ì„œë¹„ìŠ¤ ì´ìš©ì•½ê´€ í˜ì´ì§€"""
    return """
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ì„œë¹„ìŠ¤ ì´ìš©ì•½ê´€ - ë³¸ì‹ìŠ¤ëƒ…ëŸ¬</title>
        <style>
            body { font-family: 'Segoe UI', system-ui, sans-serif; line-height: 1.6; margin: 0; padding: 2rem; background: #f8f9fa; }
            .container { max-width: 800px; margin: 0 auto; background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #0d6efd; border-bottom: 2px solid #0d6efd; padding-bottom: 0.5rem; }
            h2 { color: #495057; margin-top: 2rem; }
            .date { color: #6c757d; margin-bottom: 2rem; }
            ul, ol { padding-left: 1.5rem; }
            li { margin-bottom: 0.5rem; }
            ol > li > ul { margin-top: 0.5rem; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ì„œë¹„ìŠ¤ ì´ìš©ì•½ê´€</h1>
            <p class="date">ìµœì¢… ìˆ˜ì •ì¼: 2024ë…„ 9ì›” 23ì¼</p>

            <h2>ì œ1ì¡° (ëª©ì )</h2>
            <p>ì´ ì•½ê´€ì€ ë³¸ì‹ìŠ¤ëƒ…ëŸ¬(ì´í•˜ "ì„œë¹„ìŠ¤")ì˜ ì´ìš©ì¡°ê±´ ë° ì ˆì°¨, ì„œë¹„ìŠ¤ì™€ íšŒì›ì˜ ê¶Œë¦¬, ì˜ë¬´ ë° ì±…ì„ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.</p>

            <h2>ì œ2ì¡° (ì •ì˜)</h2>
            <ul>
                <li><strong>"ì„œë¹„ìŠ¤"</strong>ë¼ í•¨ì€ ë³¸ì‹ìŠ¤ëƒ…ëŸ¬ê°€ ì œê³µí•˜ëŠ” ì›¨ë”© ìŠ¤ì¼€ì¤„ ê´€ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.</li>
                <li><strong>"íšŒì›"</strong>ì´ë¼ í•¨ì€ ì„œë¹„ìŠ¤ì— ì ‘ì†í•˜ì—¬ ì´ ì•½ê´€ì— ë”°ë¼ ì„œë¹„ìŠ¤ë¥¼ ë°›ëŠ” ê³ ê°ì„ ë§í•©ë‹ˆë‹¤.</li>
                <li><strong>"ê³„ì •"</strong>ì´ë¼ í•¨ì€ íšŒì›ì˜ ì‹ë³„ê³¼ ì„œë¹„ìŠ¤ ì´ìš©ì„ ìœ„í•˜ì—¬ íšŒì›ì´ ì„ ì •í•œ êµ¬ê¸€ ê³„ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</li>
            </ul>

            <h2>ì œ3ì¡° (ì•½ê´€ì˜ íš¨ë ¥ ë° ë³€ê²½)</h2>
            <ol>
                <li>ì´ ì•½ê´€ì€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ëª¨ë“  íšŒì›ì—ê²Œ ê·¸ íš¨ë ¥ì´ ë°œìƒí•©ë‹ˆë‹¤.</li>
                <li>ì„œë¹„ìŠ¤ëŠ” í•„ìš”í•œ ê²½ìš° ì´ ì•½ê´€ì„ ë³€ê²½í•  ìˆ˜ ìˆìœ¼ë©°, ë³€ê²½ëœ ì•½ê´€ì€ ì„œë¹„ìŠ¤ ë‚´ ê³µì§€ì‚¬í•­ì„ í†µí•´ ê³µì§€ë©ë‹ˆë‹¤.</li>
                <li>ë³€ê²½ëœ ì•½ê´€ì— ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°, íšŒì›ì€ ì„œë¹„ìŠ¤ ì´ìš©ì„ ì¤‘ë‹¨í•˜ê³  íƒˆí‡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
            </ol>

            <h2>ì œ4ì¡° (ì„œë¹„ìŠ¤ì˜ ì œê³µ ë° ë³€ê²½)</h2>
            <ol>
                <li>ì„œë¹„ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—…ë¬´ë¥¼ ì œê³µí•©ë‹ˆë‹¤:
                    <ul>
                        <li>ì›¨ë”© ìŠ¤ì¼€ì¤„ ìƒì„±, ìˆ˜ì •, ì‚­ì œ, ì¡°íšŒ</li>
                        <li>ìŠ¤ì¼€ì¤„ ë°ì´í„° í´ë¼ìš°ë“œ ë™ê¸°í™”</li>
                        <li>ìŠ¤ì¼€ì¤„ í†µê³„ ë° ë¶„ì„</li>
                        <li>ê¸°íƒ€ ì„œë¹„ìŠ¤ê°€ ì •í•˜ëŠ” ì—…ë¬´</li>
                    </ul>
                </li>
                <li>ì„œë¹„ìŠ¤ëŠ” ìš´ì˜ìƒ, ê¸°ìˆ ìƒì˜ í•„ìš”ì— ë”°ë¼ ì œê³µí•˜ê³  ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
            </ol>

            <h2>ì œ5ì¡° (ì„œë¹„ìŠ¤ ì´ìš©)</h2>
            <ol>
                <li>ì„œë¹„ìŠ¤ ì´ìš©ì€ ì—°ì¤‘ë¬´íœ´, 1ì¼ 24ì‹œê°„ì„ ì›ì¹™ìœ¼ë¡œ í•©ë‹ˆë‹¤.</li>
                <li>ì„œë¹„ìŠ¤ëŠ” ì»´í“¨í„° ë“± ì •ë³´í†µì‹ ì„¤ë¹„ì˜ ë³´ìˆ˜ì ê²€, êµì²´ ë° ê³ ì¥, í†µì‹ ì˜ ë‘ì ˆ ë“±ì˜ ì‚¬ìœ ê°€ ë°œìƒí•œ ê²½ìš°ì—ëŠ” ì„œë¹„ìŠ¤ì˜ ì œê³µì„ ì¼ì‹œì ìœ¼ë¡œ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
                <li>ë¬´ë£Œ ì„œë¹„ìŠ¤ëŠ” ì„œë¹„ìŠ¤ì˜ ì •ì±…ì— ë”°ë¼ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
            </ol>

            <h2>ì œ6ì¡° (íšŒì›ì˜ ì˜ë¬´)</h2>
            <ol>
                <li>íšŒì›ì€ ë‹¤ìŒ í–‰ìœ„ë¥¼ í•˜ì—¬ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤:
                    <ul>
                        <li>ì‹ ì²­ ë˜ëŠ” ë³€ê²½ ì‹œ í—ˆìœ„ë‚´ìš©ì˜ ë“±ë¡</li>
                        <li>íƒ€ì¸ì˜ ì •ë³´ ë„ìš©</li>
                        <li>ì„œë¹„ìŠ¤ì— ê²Œì‹œëœ ì •ë³´ì˜ ë³€ê²½</li>
                        <li>ì„œë¹„ìŠ¤ê°€ ì •í•œ ì •ë³´ ì´ì™¸ì˜ ì •ë³´(ì»´í“¨í„° í”„ë¡œê·¸ë¨ ë“±) ë“±ì˜ ì†¡ì‹  ë˜ëŠ” ê²Œì‹œ</li>
                        <li>ì„œë¹„ìŠ¤ ê¸°íƒ€ ì œ3ìì˜ ì €ì‘ê¶Œ ë“± ì§€ì ì¬ì‚°ê¶Œì— ëŒ€í•œ ì¹¨í•´</li>
                        <li>ì„œë¹„ìŠ¤ ê¸°íƒ€ ì œ3ìì˜ ëª…ì˜ˆë¥¼ ì†ìƒì‹œí‚¤ê±°ë‚˜ ì—…ë¬´ë¥¼ ë°©í•´í•˜ëŠ” í–‰ìœ„</li>
                        <li>ì™¸ì„¤ ë˜ëŠ” í­ë ¥ì ì¸ ë©”ì‹œì§€, í™”ìƒ, ìŒì„±, ê¸°íƒ€ ê³µì„œì–‘ì†ì— ë°˜í•˜ëŠ” ì •ë³´ë¥¼ ì„œë¹„ìŠ¤ì— ê³µê°œ ë˜ëŠ” ê²Œì‹œí•˜ëŠ” í–‰ìœ„</li>
                    </ul>
                </li>
            </ol>

            <h2>ì œ7ì¡° (ê°œì¸ì •ë³´ë³´í˜¸)</h2>
            <p>ì„œë¹„ìŠ¤ëŠ” ê´€ë ¨ë²•ë ¹ì´ ì •í•˜ëŠ” ë°”ì— ë”°ë¼ íšŒì›ì˜ ê°œì¸ì •ë³´ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•©ë‹ˆë‹¤. ê°œì¸ì •ë³´ì˜ ë³´í˜¸ ë° ì‚¬ìš©ì— ëŒ€í•´ì„œëŠ” ê´€ë ¨ë²•ë ¹ ë° ì„œë¹„ìŠ¤ì˜ ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ì´ ì ìš©ë©ë‹ˆë‹¤.</p>

            <h2>ì œ8ì¡° (ë©´ì±…ì¡°í•­)</h2>
            <ol>
                <li>ì„œë¹„ìŠ¤ëŠ” ì²œì¬ì§€ë³€ ë˜ëŠ” ì´ì— ì¤€í•˜ëŠ” ë¶ˆê°€í•­ë ¥ìœ¼ë¡œ ì¸í•˜ì—¬ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” ì„œë¹„ìŠ¤ ì œê³µì— ê´€í•œ ì±…ì„ì´ ë©´ì œë©ë‹ˆë‹¤.</li>
                <li>ì„œë¹„ìŠ¤ëŠ” íšŒì›ì˜ ê·€ì±…ì‚¬ìœ ë¡œ ì¸í•œ ì„œë¹„ìŠ¤ ì´ìš©ì˜ ì¥ì• ì— ëŒ€í•˜ì—¬ ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.</li>
                <li>ì„œë¹„ìŠ¤ëŠ” íšŒì›ì´ ì„œë¹„ìŠ¤ì— ê²Œì¬í•œ ì •ë³´, ìë£Œ, ì‚¬ì‹¤ì˜ ì‹ ë¢°ë„, ì •í™•ì„± ë“±ì˜ ë‚´ìš©ì— ê´€í•˜ì—¬ëŠ” ì±…ì„ì„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.</li>
            </ol>

            <h2>ì œ9ì¡° (ì¤€ê±°ë²• ë° ê´€í• ë²•ì›)</h2>
            <p>ì´ ì•½ê´€ì— ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬í•­ì€ ëŒ€í•œë¯¼êµ­ì˜ ê´€ë ¨ ë²•ë ¹ì— ì˜í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ ì´ìš©ìœ¼ë¡œ ë°œìƒí•œ ë¶„ìŸì— ëŒ€í•´ ì†Œì†¡ì´ ì œê¸°ë˜ëŠ” ê²½ìš° ê´€ë ¨ ë²•ë ¹ì— ë”°ë¥¸ ë²•ì›ì„ ê´€í•  ë²•ì›ìœ¼ë¡œ í•©ë‹ˆë‹¤.</p>

            <div style="margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #dee2e6; text-align: center; color: #6c757d; font-size: 0.875rem;">
                ë¶€ì¹™: ì´ ì•½ê´€ì€ 2024ë…„ 9ì›” 23ì¼ë¶€í„° ì ìš©ë©ë‹ˆë‹¤.
            </div>
        </div>
    </body>
    </html>
    """
# ============================================================================
# V2 RESTful API Endpoints
# ============================================================================

@app.get("/api/schedules")
def get_schedules(
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Get all schedules for a user"""
    try:
        schedules = db.query(Schedule).filter(Schedule.user_id == user_id).all()

        # Convert to response format
        result = []
        for schedule in schedules:
            result.append({
                'id': str(schedule.id),
                'date': schedule.date,
                'time': schedule.time,
                'location': schedule.location,
                'couple': schedule.couple or "",
                'contact': schedule.contact or "",
                'brand': schedule.brand or "",
                'album': schedule.album or "",
                'photographer': schedule.photographer or "",
                'cuts': schedule.cuts or 0,
                'price': schedule.price or 0,
                'manager': schedule.manager or "",
                'memo': schedule.memo or "",
                'photoNote': schedule.photo_note,
                'isDuplicate': schedule.needs_review,
                'createdAt': schedule.created_at.isoformat() if schedule.created_at else None,
                'updatedAt': schedule.updated_at.isoformat() if schedule.updated_at else None,
            })

        return result
        
    except Exception as e:
        logger.error(f"Failed to get schedules: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/schedules")
def create_schedule(
    schedule: Dict,
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Create a new schedule"""
    try:
        new_schedule = Schedule(
            user_id=user_id,
            date=schedule.get('date', ''),
            time=schedule.get('time', ''),
            location=schedule.get('location', ''),
            couple=schedule.get('couple', ''),
            brand=schedule.get('brand', ''),
            cuts=schedule.get('cuts', 0),
            price=schedule.get('price', 0),
            manager=schedule.get('manager', ''),
            memo=schedule.get('memo', ''),
            needs_review=schedule.get('isDuplicate', False),
        )

        db.add(new_schedule)
        db.commit()
        db.refresh(new_schedule)

        return {
            'id': str(new_schedule.id),
            'date': new_schedule.date,
            'time': new_schedule.time,
            'location': new_schedule.location,
            'couple': new_schedule.couple,
            'cuts': new_schedule.cuts,
            'price': new_schedule.price,
            'manager': new_schedule.manager,
            'brand': new_schedule.brand,
            'memo': new_schedule.memo,
            'isDuplicate': new_schedule.needs_review,
            'createdAt': new_schedule.created_at.isoformat() if new_schedule.created_at else None,
            'updatedAt': new_schedule.updated_at.isoformat() if new_schedule.updated_at else None,
        }
        
    except Exception as e:
        logger.error(f"Failed to create schedule: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/schedules/{schedule_id}")
def update_schedule(
    schedule_id: str,
    schedule: Dict,
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Update a schedule"""
    try:
        print(f"ğŸ”„ Update schedule {schedule_id}, keys: {schedule.keys()}")
        if 'photoNote' in schedule:
            print(f"ğŸ“ PhotoNote data: {schedule['photoNote']}")

        existing = db.query(Schedule).filter(
            Schedule.id == int(schedule_id),
            Schedule.user_id == user_id
        ).first()

        if not existing:
            raise HTTPException(status_code=404, detail="Schedule not found")
        
        # Update fields
        if 'date' in schedule:
            existing.date = schedule['date']
        if 'time' in schedule:
            existing.time = schedule['time']
        if 'location' in schedule:
            existing.location = schedule['location']
        if 'couple' in schedule:
            existing.couple = schedule['couple']
        if 'contact' in schedule:
            existing.contact = schedule['contact']
        if 'brand' in schedule:
            existing.brand = schedule['brand']
        if 'album' in schedule:
            existing.album = schedule['album']
        if 'photographer' in schedule:
            existing.photographer = schedule['photographer']
        if 'cuts' in schedule:
            existing.cuts = schedule['cuts']
        if 'price' in schedule:
            existing.price = schedule['price']
        if 'manager' in schedule:
            existing.manager = schedule['manager']
        if 'memo' in schedule:
            existing.memo = schedule['memo']
        if 'photoNote' in schedule:
            existing.photo_note = schedule['photoNote']
        if 'isDuplicate' in schedule:
            existing.needs_review = schedule['isDuplicate']

        # Auto-create tags if brand or album was updated
        if 'brand' in schedule or 'album' in schedule:
            auto_create_tags_from_schedule(db, user_id, existing.brand, existing.album)

        db.commit()
        db.refresh(existing)

        return {
            'id': str(existing.id),
            'date': existing.date,
            'time': existing.time,
            'location': existing.location,
            'couple': existing.couple,
            'contact': existing.contact,
            'brand': existing.brand,
            'album': existing.album,
            'photographer': existing.photographer,
            'cuts': existing.cuts,
            'price': existing.price,
            'manager': existing.manager,
            'memo': existing.memo,
            'photoNote': existing.photo_note,
            'isDuplicate': existing.needs_review,
            'createdAt': existing.created_at.isoformat() if existing.created_at else None,
            'updatedAt': existing.updated_at.isoformat() if existing.updated_at else None,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update schedule: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/schedules/{schedule_id}")
def delete_schedule(
    schedule_id: str,
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Delete a schedule"""
    try:
        existing = db.query(Schedule).filter(
            Schedule.id == int(schedule_id),
            Schedule.user_id == user_id
        ).first()
        
        if not existing:
            raise HTTPException(status_code=404, detail="Schedule not found")
        
        db.delete(existing)
        db.commit()
        
        return {"success": True, "message": "Schedule deleted"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete schedule: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/schedules/migrate")
def migrate_schedules(
    from_user_id: str = Query(..., description="Anonymous user ID"),
    to_user_id: str = Query(..., description="Target authenticated user ID"),
    db: Session = Depends(get_database)
):
    """Migrate all schedules and tags from anonymous user to authenticated user"""
    try:
        print(f"ğŸ”„ Migrating data from {from_user_id} to {to_user_id}")

        # Check if source user has any schedules
        schedule_count = db.query(Schedule).filter(Schedule.user_id == from_user_id).count()
        if schedule_count == 0:
            return {
                "success": True,
                "message": "No schedules to migrate",
                "migrated_schedules": 0,
                "migrated_tags": 0
            }

        # Migrate schedules
        db.query(Schedule).filter(Schedule.user_id == from_user_id).update(
            {"user_id": to_user_id},
            synchronize_session=False
        )

        # Migrate tags
        tag_count = db.query(Tag).filter(Tag.user_id == from_user_id).count()
        db.query(Tag).filter(Tag.user_id == from_user_id).update(
            {"user_id": to_user_id},
            synchronize_session=False
        )

        db.commit()

        print(f"âœ… Migration complete: {schedule_count} schedules, {tag_count} tags")

        return {
            "success": True,
            "message": f"Successfully migrated {schedule_count} schedules and {tag_count} tags",
            "migrated_schedules": schedule_count,
            "migrated_tags": tag_count
        }

    except Exception as e:
        logger.error(f"Failed to migrate schedules: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/schedules/batch")
def batch_create_schedules(
    schedules: List[Dict] = Body(..., embed=True),
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Batch create schedules"""
    try:
        created_schedules = []

        for schedule in schedules:
            new_schedule = Schedule(
                user_id=user_id,
                date=schedule.get('date', ''),
                time=schedule.get('time', ''),
                location=schedule.get('location', ''),
                couple=schedule.get('couple', ''),
                brand=schedule.get('brand', ''),
                album=schedule.get('album', ''),
                photographer=schedule.get('photographer', ''),
                contact=schedule.get('contact', ''),
                cuts=schedule.get('cuts', 0),
                price=schedule.get('price', 0),
                manager=schedule.get('manager', ''),
                memo=schedule.get('memo', ''),
                needs_review=schedule.get('isDuplicate', False),
            )

            db.add(new_schedule)
            db.flush()  # Get ID without committing

            print(f"ğŸ“ Created schedule - couple: '{new_schedule.couple}'")

            # Add to result list
            created_schedules.append({
                'id': str(new_schedule.id),
                'date': new_schedule.date,
                'time': new_schedule.time,
                'location': new_schedule.location,
                'couple': new_schedule.couple,
                'cuts': new_schedule.cuts,
                'price': new_schedule.price,
                'photographer': new_schedule.photographer,
                'contact': new_schedule.contact,
                'album': new_schedule.album,
                'brand': new_schedule.brand,
                'manager': new_schedule.manager,
                'memo': new_schedule.memo,
                'isDuplicate': new_schedule.needs_review,
                'createdAt': new_schedule.created_at.isoformat() if new_schedule.created_at else None,
                'updatedAt': new_schedule.updated_at.isoformat() if new_schedule.updated_at else None,
            })

        db.commit()

        return created_schedules

    except Exception as e:
        logger.error(f"Failed to batch create schedules: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/schedules/batch-delete")
def batch_delete_schedules(
    ids: List[str] = Body(..., embed=True),
    user_id: str = Query(..., description="User ID"),
    db: Session = Depends(get_database)
):
    """Batch delete schedules"""
    try:

        for schedule_id in ids:
            existing = db.query(Schedule).filter(
                Schedule.id == int(schedule_id),
                Schedule.user_id == user_id
            ).first()

            if existing:
                db.delete(existing)

        db.commit()

        return {"success": True, "message": f"Deleted {len(ids)} schedules"}

    except Exception as e:
        logger.error(f"Failed to batch delete schedules: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))

# ==================== Tag Helper Functions ====================

def auto_create_tags_from_schedule(db_session, user_id: str, brand: str, album: str):
    """ìŠ¤ì¼€ì¤„ ì €ì¥/ì—…ë°ì´íŠ¸ ì‹œ ìë™ìœ¼ë¡œ íƒœê·¸ ìƒì„±"""
    from database import Tag
    import re

    created_tags = []

    # ë¸Œëœë“œ íƒœê·¸ ìƒì„±
    if brand and brand.strip():
        brand_value = re.sub(r'\s+', ' ', brand.strip())

        existing = db_session.query(Tag).filter(
            Tag.user_id == user_id,
            Tag.tag_type == 'brand',
            Tag.tag_value == brand_value
        ).first()

        if not existing:
            new_tag = Tag(user_id=user_id, tag_type='brand', tag_value=brand_value)
            db_session.add(new_tag)
            created_tags.append(('brand', brand_value))

    # ì•¨ë²” íƒœê·¸ ìƒì„±
    if album and album.strip():
        album_value = re.sub(r'\s+', ' ', album.strip())

        existing = db_session.query(Tag).filter(
            Tag.user_id == user_id,
            Tag.tag_type == 'album',
            Tag.tag_value == album_value
        ).first()

        if not existing:
            new_tag = Tag(user_id=user_id, tag_type='album', tag_value=album_value)
            db_session.add(new_tag)
            created_tags.append(('album', album_value))

    return created_tags

# ==================== Tag API ====================

@app.get("/api/tags/{user_id}")
async def get_tags(user_id: str, tag_type: Optional[str] = None):
    """ì‚¬ìš©ìì˜ íƒœê·¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        db_session = next(get_database())
        from database import Tag

        query = db_session.query(Tag).filter(Tag.user_id == user_id)

        if tag_type:
            query = query.filter(Tag.tag_type == tag_type)

        tags = query.order_by(Tag.tag_value).all()

        return {
            "success": True,
            "tags": [tag.to_dict() for tag in tags]
        }

    except Exception as e:
        logger.error(f"âŒ Get tags error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/tags/{user_id}")
async def create_tag(user_id: str, tag_data: dict):
    """ìƒˆ íƒœê·¸ ìƒì„±"""
    try:
        db_session = next(get_database())
        from database import Tag

        tag_type = tag_data.get('tag_type')
        tag_value = tag_data.get('tag_value', '').strip()

        if not tag_type or not tag_value:
            raise HTTPException(status_code=400, detail="tag_type and tag_value are required")

        if tag_type not in ['brand', 'album']:
            raise HTTPException(status_code=400, detail="tag_type must be 'brand' or 'album'")

        # ê³µë°± ì •ê·œí™”
        import re
        tag_value = re.sub(r'\s+', ' ', tag_value)

        # ì¤‘ë³µ ì²´í¬
        existing = db_session.query(Tag).filter(
            Tag.user_id == user_id,
            Tag.tag_type == tag_type,
            Tag.tag_value == tag_value
        ).first()

        if existing:
            return {"success": True, "tag": existing.to_dict(), "created": False}

        # ìƒˆ íƒœê·¸ ìƒì„±
        new_tag = Tag(
            user_id=user_id,
            tag_type=tag_type,
            tag_value=tag_value
        )

        db_session.add(new_tag)
        db_session.commit()
        db_session.refresh(new_tag)

        return {"success": True, "tag": new_tag.to_dict(), "created": True}

    except HTTPException:
        raise
    except Exception as e:
        db_session.rollback()
        logger.error(f"âŒ Create tag error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/tags/{user_id}/{tag_id}")
async def delete_tag(user_id: str, tag_id: int):
    """íƒœê·¸ ì‚­ì œ ë° ê´€ë ¨ ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸"""
    try:
        db_session = next(get_database())
        from database import Tag, Schedule

        # íƒœê·¸ ì¡°íšŒ
        tag = db_session.query(Tag).filter(
            Tag.id == tag_id,
            Tag.user_id == user_id
        ).first()

        if not tag:
            raise HTTPException(status_code=404, detail="Tag not found")

        # ê´€ë ¨ ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸ (í•´ë‹¹ íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤ì¼€ì¤„ì˜ í•„ë“œë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ)
        field_name = tag.tag_type  # 'brand' or 'album'
        affected_schedules = db_session.query(Schedule).filter(
            Schedule.user_id == user_id,
            getattr(Schedule, field_name) == tag.tag_value
        ).all()

        for schedule in affected_schedules:
            setattr(schedule, field_name, '')

        # íƒœê·¸ ì‚­ì œ
        db_session.delete(tag)
        db_session.commit()

        return {
            "success": True,
            "deleted_tag": tag.to_dict(),
            "affected_schedules": len(affected_schedules)
        }

    except HTTPException:
        raise
    except Exception as e:
        db_session.rollback()
        logger.error(f"âŒ Delete tag error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/tags/{user_id}/sync")
async def sync_tags_from_schedules(user_id: str):
    """ê¸°ì¡´ ìŠ¤ì¼€ì¤„ ë°ì´í„°ì—ì„œ íƒœê·¸ ì¶”ì¶œ ë° ë™ê¸°í™” (ë°°ì¹˜ ìµœì í™”)"""
    try:
        db_session = next(get_database())
        from database import Tag, Schedule
        import re

        # 1. ê¸°ì¡´ íƒœê·¸ë¥¼ í•œ ë²ˆì— ëª¨ë‘ ê°€ì ¸ì˜¤ê¸° (ë©”ëª¨ë¦¬ì— ìºì‹±)
        existing_tags = db_session.query(Tag).filter(Tag.user_id == user_id).all()
        existing_tag_set = {(tag.tag_type, tag.tag_value) for tag in existing_tags}

        # 2. ëª¨ë“  ìŠ¤ì¼€ì¤„ì—ì„œ ê³ ìœ í•œ íƒœê·¸ ì¶”ì¶œ
        schedules = db_session.query(Schedule).filter(Schedule.user_id == user_id).all()
        unique_tags = set()

        for schedule in schedules:
            # ë¸Œëœë“œ íƒœê·¸
            if schedule.brand and schedule.brand.strip():
                brand_value = re.sub(r'\s+', ' ', schedule.brand.strip())
                unique_tags.add(('brand', brand_value))

            # ì•¨ë²” íƒœê·¸
            if schedule.album and schedule.album.strip():
                album_value = re.sub(r'\s+', ' ', schedule.album.strip())
                unique_tags.add(('album', album_value))

        # 3. DBì— ì—†ëŠ” ìƒˆ íƒœê·¸ë§Œ ì¶”ê°€ (ë©”ëª¨ë¦¬ ë¹„êµ)
        new_tags = unique_tags - existing_tag_set
        created_tags = []

        for tag_type, tag_value in new_tags:
            new_tag = Tag(user_id=user_id, tag_type=tag_type, tag_value=tag_value)
            db_session.add(new_tag)
            created_tags.append(tag_value)

        db_session.commit()

        return {
            "success": True,
            "created_count": len(created_tags),
            "created_tags": created_tags
        }

    except Exception as e:
        db_session.rollback()
        logger.error(f"âŒ Sync tags error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
